from .account_transaction import AccountTransaction
from .person import Person
from .classification import Classification
from .installment import Installment
from ....agents import EmbeddingAgent
from ....agents.chat_manager import chat_manager
from pgvector.django import L2Distance
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage, AIMessage
import os
from typing import List, Tuple, Optional

GEMINI_API_KEY = os.environ.get('GEMINI_API_KEY')

def query_semantic_rag(question: str, top_k: int = 5) -> str:
    """
    Executa busca semÃ¢ntica (RAG) com contexto RICO e gera resposta usando Gemini.
    
    Args:
        question: Pergunta do usuÃ¡rio
        top_k: NÃºmero de transaÃ§Ãµes similares a retornar (padrÃ£o: 5)
    
    Returns:
        Resposta gerada pelo LLM com base no contexto encontrado
    
    Exemplos de perguntas que FUNCIONAM com contexto rico:
    - "Quanto gastei com a IGUACU MAQUINAS?"
    - "Quais notas fiscais sÃ£o de manutenÃ§Ã£o?"
    - "Mostre transaÃ§Ãµes acima de R$ 3000"
    - "Quais fornecedores tenho cadastrados?"
    - "Qual o total de despesas com INSUMOS AGRÃCOLAS?"
    """
    
    print(f"\n Buscando por: '{question}'")

    # Gera embedding da pergunta do usuÃ¡rio usando EmbeddingAgent
    embedding_agent = EmbeddingAgent()
    query_vector = embedding_agent.generate_embedding(question)

    if query_vector is None:
        return "NÃ£o foi possÃ­vel processar sua pergunta. Tente reformular."
    
    # Busca de transaÃ§Ãµes semelhantes usando o embedding da descriÃ§Ã£o
    similar_transactions = AccountTransaction.objects.filter(
        descricao_embedding__isnull=False
    ).order_by(
        L2Distance('descricao_embedding', query_vector)
    ).prefetch_related(
        'fornecedor_cliente',
        'faturado', 
        'classificacoes',
        'parcelas'
    )[:top_k]
    
    if not similar_transactions:
        return "NÃ£o encontrei nenhuma transaÃ§Ã£o no banco de dados que corresponda Ã  sua pergunta."
    
    print(f"Encontradas {len(similar_transactions)} transaÃ§Ãµes relevantes\n")
    
    # Monstagem de contexto
    context = "DADOS ENCONTRADOS NO BANCO DE DADOS:\n\n"    
    for idx, tx in enumerate(similar_transactions, 1):
        classificacoes = ", ".join([c.descricao for c in tx.classificacoes.all()])
        parcelas = tx.parcelas.all()
        total_parcelas = parcelas.count()
        parcelas_abertas = parcelas.filter(status_parcela='aberta').count()
        
        context += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TRANSAÃ‡ÃƒO #{idx} - ID: {tx.id}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Nota Fiscal: {tx.numero_nota_fiscal}
Data de EmissÃ£o: {tx.data_emissao.strftime('%d/%m/%Y')}
Valor Total: R$ {tx.valor_total:.2f}
Fornecedor: {tx.fornecedor_cliente.razao_social}
Faturado: {tx.faturado.razao_social}
DescriÃ§Ã£o: {tx.descricao}
ClassificaÃ§Ãµes: {classificacoes or 'NÃ£o especificado'}
Parcelas: {total_parcelas} total ({parcelas_abertas} abertas)
Status: {tx.get_status_display()}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""
    
    # GeraÃ§Ã£o de resposta com LLM
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=0.1,
        google_api_key=GEMINI_API_KEY)
    
    prompt_template = """
VocÃª Ã© um assistente financeiro especializado em anÃ¡lise de notas fiscais e despesas.

INSTRUÃ‡Ã•ES IMPORTANTES:
1. Responda APENAS com base no contexto fornecido abaixo
2. Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga claramente "NÃ£o encontrei essa informaÃ§Ã£o nos dados disponÃ­veis"
3. Quando falar sobre valores, sempre use formataÃ§Ã£o brasileira (R$ 1.234,56)
4. Seja claro, objetivo e organizado
5. Se houver mÃºltiplas transaÃ§Ãµes, organize a resposta de forma estruturada
6. Quando relevante, mostre totais e resumos
7. NÃ£o analise dados com status='inativo'

CONTEXTO (Dados do Banco de Dados):
{context}

PERGUNTA DO USUÃRIO:
{question}

RESPOSTA:
"""
    
    prompt = ChatPromptTemplate.from_template(prompt_template)
    
    rag_chain = (
        prompt
        | llm
        | StrOutputParser()
    )
    
    try:
        answer = rag_chain.invoke({
            "context": context,
            "question": question
        })
        
        print(f"Resposta gerada com sucesso!\n")
        return answer
        
    except Exception as e:
        print(f"Erro ao gerar resposta: {e}")
        return f"Erro ao processar sua pergunta: {str(e)}"


def query_semantic_rag_with_history(
    question: str,
    session_id: Optional[str] = None,
    top_k: int = 5
) -> dict:
    """
    Executa busca semÃ¢ntica (RAG) com histÃ³rico de conversa.
    MantÃ©m o contexto entre mÃºltiplas mensagens.

    Args:
        question: Pergunta do usuÃ¡rio
        session_id: ID da sessÃ£o de chat (opcional)
        top_k: NÃºmero de transaÃ§Ãµes similares a retornar (padrÃ£o: 5)

    Returns:
        dict: Resposta, session_id e metadados
    """
    print(f"\nğŸ” Buscando com histÃ³rico por: '{question}'")

    # 1. Recupera ou cria histÃ³rico de chat
    chat_history = []
    is_new_session = False

    if session_id:
        session = chat_manager.get_session(session_id)
        if session and session["agent_type"] == "embedding":
            # Recupera histÃ³rico existente
            chat_history = session.get("chat", {}).get("history", [])
            print(f"âœ“ SessÃ£o existente recuperada: {session_id} ({len(chat_history)} mensagens)")
        else:
            print(f"âš  SessÃ£o invÃ¡lida ou expirada: {session_id}")
            session_id = None

    if not session_id:
        print("âœ“ Criando nova sessÃ£o")
        is_new_session = True

    # 2. Gera embedding da pergunta
    embedding_agent = EmbeddingAgent()
    query_vector = embedding_agent.generate_embedding(question)

    if query_vector is None:
        return {
            "response": "NÃ£o foi possÃ­vel processar sua pergunta. Tente reformular.",
            "error": "Falha ao gerar embedding",
            "session_id": session_id,
            "is_new_session": is_new_session
        }

    # 3. Busca transaÃ§Ãµes similares
    similar_transactions = AccountTransaction.objects.filter(
        descricao_embedding__isnull=False
    ).order_by(
        L2Distance('descricao_embedding', query_vector)
    ).prefetch_related(
        'fornecedor_cliente',
        'faturado',
        'classificacoes',
        'parcelas'
    )[:top_k]

    if not similar_transactions:
        return {
            "response": "NÃ£o encontrei nenhuma transaÃ§Ã£o no banco de dados que corresponda Ã  sua pergunta.",
            "error": None,
            "session_id": session_id,
            "is_new_session": is_new_session
        }

    print(f"âœ“ Encontradas {len(similar_transactions)} transaÃ§Ãµes relevantes")

    # 4. Monta contexto das transaÃ§Ãµes
    context = "DADOS ENCONTRADOS NO BANCO DE DADOS:\n\n"
    for idx, tx in enumerate(similar_transactions, 1):
        classificacoes = ", ".join([c.descricao for c in tx.classificacoes.all()])
        parcelas = tx.parcelas.all()
        total_parcelas = parcelas.count()
        parcelas_abertas = parcelas.filter(status_parcela='aberta').count()

        context += f"""
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
TRANSAÃ‡ÃƒO #{idx} - ID: {tx.id}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Nota Fiscal: {tx.numero_nota_fiscal}
Data de EmissÃ£o: {tx.data_emissao.strftime('%d/%m/%Y')}
Valor Total: R$ {tx.valor_total:.2f}
Fornecedor: {tx.fornecedor_cliente.razao_social}
Faturado: {tx.faturado.razao_social}
DescriÃ§Ã£o: {tx.descricao}
ClassificaÃ§Ãµes: {classificacoes or 'NÃ£o especificado'}
Parcelas: {total_parcelas} total ({parcelas_abertas} abertas)
Status: {tx.get_status_display()}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

"""

    # 5. Monta prompt com histÃ³rico
    system_prompt = """VocÃª Ã© um assistente financeiro especializado em anÃ¡lise de notas fiscais e despesas.

INSTRUÃ‡Ã•ES IMPORTANTES:
1. Responda APENAS com base no contexto fornecido abaixo
2. Use o histÃ³rico da conversa para entender referÃªncias (ex: "a primeira opÃ§Ã£o", "aquele fornecedor")
3. Se a informaÃ§Ã£o nÃ£o estiver no contexto, diga claramente "NÃ£o encontrei essa informaÃ§Ã£o nos dados disponÃ­veis"
4. Quando falar sobre valores, sempre use formataÃ§Ã£o brasileira (R$ 1.234,56)
5. Seja claro, objetivo e organizado
6. Se houver mÃºltiplas transaÃ§Ãµes, organize a resposta de forma estruturada
7. Quando relevante, mostre totais e resumos
8. NÃ£o analise dados com status='inativo'

CONTEXTO (Dados do Banco de Dados):
{context}
"""

    # 6. Cria LLM e chain com histÃ³rico
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash-lite",
        temperature=0.1,
        google_api_key=GEMINI_API_KEY
    )

    # Prepara mensagens com histÃ³rico
    messages = [{"role": "system", "content": system_prompt.format(context=context)}]

    # Adiciona histÃ³rico anterior
    for msg in chat_history:
        messages.append(msg)

    # Adiciona pergunta atual
    messages.append({"role": "user", "content": question})

    try:
        # Gera resposta
        response = llm.invoke(messages)
        answer = response.content

        # Atualiza histÃ³rico
        chat_history.append({"role": "user", "content": question})
        chat_history.append({"role": "assistant", "content": answer})

        # Salva sessÃ£o
        if is_new_session:
            session_data = {
                "history": chat_history,
                "embedding_agent": True
            }
            new_session_id = chat_manager.create_session(session_data, agent_type="embedding")
        else:
            new_session_id = session_id
            session = chat_manager.get_session(new_session_id)
            if session:
                session["chat"]["history"] = chat_history
                chat_manager.increment_message_count(new_session_id)

        print(f"âœ“ Resposta gerada com sucesso! Session: {new_session_id}")

        return {
            "response": answer,
            "error": None,
            "session_id": new_session_id,
            "is_new_session": is_new_session,
            "transactions_found": len(similar_transactions)
        }

    except Exception as e:
        print(f"âŒ Erro ao gerar resposta: {e}")
        return {
            "response": f"Erro ao processar sua pergunta: {str(e)}",
            "error": str(e),
            "session_id": session_id,
            "is_new_session": is_new_session
        }
